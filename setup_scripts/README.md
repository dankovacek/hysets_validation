Data Acquisition and Preprocessing
==================================

Set up Cloud Computing
----------------------

(Or local machine if it’s beefy enough, &gt;=128GB ram required!).
**Assumes installation on Ubuntu Linux because it’s commonly offered in
cloud compute services.** These instructions are intended to minimize
setup time and costs associated with hourly compute resources (~$1-3 per
hour on [DigitalOcean](https://www.digitalocean.com/pricing/calculator/)
for 128 GB RAM, 12 core processor.)

**Install GDAL**

> `sudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable`  
> `sudo apt-get update`

Install software utilities:
&gt;`sudo apt-get install libgdal-dev gdal-bin libproj15 libproj19 libproj-dev openmpi-bin libopenmpi-dev libboost-iostreams-dev parallel unzip dos2unix zip`

**Clone the repository (from the root directory)**

> `git clone https://github.com/dankovacek/hysets_validation`

### Install Python package manager (pip)

If not automatically installed, install Python and virtualenv (assuming
Python3 is installed by default on a linux distribution):

> `sudo apt install python3.8-venv pip`

Create Python 3.8+ virtual environment at the root level directory:

> `python3 -m venv env/`

Activate the virual environment:  
&gt;`source env/bin/activate`

Install Python packages:  
&gt;`pip install -r requirements.txt`

Download Source Data
--------------------

### HYSETS

From the project directory (`hysets_validation/`), create a directory to
store the HYSETS study data. The full repository is ~14.6 GB and can be
accessed [here](https://osf.io/rpc3w/). However, for validation we only
need the results file and the basins geometry. The results file is
included in the repo, and we can access the basin archive file directly
from Google Drive:

> `mkdir source_data/HYSETS_data/` `cd source_data/HYSETS_data/`  
> `curl -O https://files.osf.io/v1/resources/rpc3w/providers/googledrive/?zip=HYSETS_watershed_boundaries.zip`  
> `mkdir USGS_basins/`  
> `unzip HYSETS_watershed_boundaries.zip -d USGS_basins/`

This file (~15GB) could take 15-20 minutes on it’s own to download, so
open a second ssh connection and continue the setup process while this
file is downloading.

It appears as though the pour points are not specified for the polygons
derived for this study, so the station locations have to be derived from
WSC and USGS data. WSC pour points are provided in the WSC basin data
that’s [collected in subsequent
steps](#markdown-header-wsc-hydrometric-station-catchment-polygons-and-metadata),
and the US-based station locations can be [downloaded from the
USGS](https://water.usgs.gov/GIS/metadata/usgswrd/XML/streamgages.xml)
by the following command:

> `cd source_data/HYSETS_data/`  
> `mkdir USGS_station_locations/`  
> `curl -O https://water.usgs.gov/GIS/dsdl/USGS_Streamgages-NHD_Locations_Shape.zip > USGS_Streamgages-NHD_Locations_Shape.zip`  
> `unzip USGS_Streamgages-NHD_Locations_Shape.zip -d USGS_station_locations/`

Note that these are station locations and not pour points, but they are
used by the basin delineation code as a first approximation in a
“snap-to-nearest” process.

### DEM Data (from USGS)

The folder `setup_scripts/` contains two files
(`1-1arcsecond-PNW-files.txt`, `2-2arcsecond-files.txt`) of links to DEM
data covering the study area [provided by the
USGS](https://apps.nationalmap.gov/downloader/). Unfortunately the
1-arcsecond set contains several small gaps near the Yukon-Alaska
border, so we need to import the 2 arcsecond data to fill in the gaps.
We can combine the two sources and specify the highest resolution where
available automatically using gdal. This is left here so the process can
be replicated for arbitrary regions. File lists can be generated by
visiting the link above, selecting a region of interest, and generating
a list of DEM files corresponding to the layers of interest.

Make two source directories for DEM data and raw files. At the root
directory level:

> `cd hysets_validation/source_data`  
> `mkdir dem_data/ dem_data/dem_files`

The custom file lists I generated for BC/AK/WA are provided in the
following folder: &gt;`cd hysets_validation/setup_scripts/file_lists/`

There are two files because I needed two different layers. Merge these
filenames into a single file:

> `cat 1-1arcsecond-PNW-files.txt 2-2arcsecond-files.txt > merged-files.txt`

You’ll likely have a carriage return added to urls (%0D) that needs to
be removed (Windows issue that seems to pop up in cloud instance):  
&gt;`dos2unix merged-files.txt`

and download files in parallel (~ 18GB total. Check what folder you are
in when executing, you should be in `hysets_validation/`):

> `cat setup_scripts/file_lists/merged-files.txt | parallel --gnu "wget -P source_data/dem_data/dem_files/ {}"`

Create a mosaic (.vrt) of the DEM files covering the Pacific North West
called `BC_DEM_mosaic_4326.vrt`. This is the main DEM index that will be
called in the analysis. Note that if the vrt srs (-a\_srs flag) is not
set, the default will be EPSG 4269 which appears to be incorrect.
Setting 4326 appears to produce properly aligned geometry.

> `cd hysets_validation/source_data/dem_data/`  
> `gdalbuildvrt -resolution highest -a_srs epsg:4326 BC_DEM_mosaic_4326.vrt dem_files/USGS_1_*.tif`

The resulting .vrt should look like this:

![DEM Mosaic of BC and administrative boundary
regions](../images/BC_dem_raw.png)

### National Hydrographic Network (NHN)

![Regions of the National Hydrographic
Network](../images/nhn_regions_full.png)

The [national blue-line
network](https://open.canada.ca/data/en/dataset/a4b190fe-e090-4e6d-881e-b87956c07977)
is a vector-based representation of surface water in Canada. The index
contains 1341 polygons representing “Work Unit Limits” (WLU) based on
sub-sub drainage areas. Here we will filter the polygons to find all
those intersecting with the polygon describing the BC provincial border,
and then group the work units into larger regions representing the major
drainage basins covering the province. The purpose of merging into
larger groups is to avoid having polygons break up continuous rivers, as
the subsequent step is to calculate flow accumulation networks for all
basins covering the province. Hydrographic features covering the study
area will be downloaded in the script that follows.

Map the group basins into the minimum covering set. Use the script found
in `setup_scripts/`. (ensure the virtual environment created above is
activated from the step above, you should see `(env) root@...` in your
terminal).

> `cd setup_scripts`  
> `python process_hydrologic_regions.py`

The script will download the NHN file containing geometries
corresponding to hydrographic features in BC. The script then takes the
sub-sub-drainage polygons and converts them to hydraulically separate
regions, in other words polygons do not cut flow paths anywhere in the
province. See the images below for original and processed data polygons:

![Original WSC sub-sub-drainage regions](img/wsc-ssda.png) ![Grouped
Major Drainage Basins after processing](img/wsc-ssda-processed.png)

Once the major groups are formed, the file
`/processed_data/merged_basin_groups/BC_basin_region_groups_EPSG4326.geojson`
should have been created. For each of the regional basins, create a
clipped DEM and reproject to EPSG:3005.  
&gt;`cd setup_scripts/`  
&gt;`python dem_basin_mapper.py`

The DEM processing will transform the raw DEM mosaic (below-left) to the
processed, trimmed and transformed to BC Albers 3005 projection, and
grouped into 18 major basins and drainage regions described above
(below-right).

![Comparision of raw and processed
DEM](../images/processed_dem_comparison.png)

Additional information on trimming ocean out of coastline for southeast
Alaska, BC, and Washington state is included in the [Trim Coastline Dem
section below](#markdown-header-trim-coastline-dem). Additional details
about the NHN are also provided in the [Additional Notes section
below](#markdown-header-nhn-additional-notes).

### WSC Hydrometric Station Catchment Polygons and Metadata

Updated set of basin polygons from WSC, published in December 2021.

> `mkdir source_data/WSC_data`  
> `cd source_data/WSC_data`  
> `wget https://collaboration.cmc.ec.gc.ca/cmc/hydrometrics/www/HydrometricNetworkBasinPolygons/07.zip`  
> `wget https://collaboration.cmc.ec.gc.ca/cmc/hydrometrics/www/HydrometricNetworkBasinPolygons/08.zip`  
> `wget https://collaboration.cmc.ec.gc.ca/cmc/hydrometrics/www/HydrometricNetworkBasinPolygons/09.zip`  
> `wget https://collaboration.cmc.ec.gc.ca/cmc/hydrometrics/www/HydrometricNetworkBasinPolygons/10.zip`

The `source_data` folder contains the WSC station metadata file of
active and historic hydrometric stations `WSC_Stations_2020.csv`.

Merge all files into one zip and create a geojson file – geopandas can
read zip files of polygons:

> `mkdir all`  
> `for x in *.zip; do unzip -d all -o -u $x ; done`  
> `zip -r WSC_basins.zip all`  
> `cd all/`  
> `for dir in */; do mkdir -- "$dir"{basin,pour_point,station}; done`  
> `for dir in */; do mv "$dir"/*DrainageBasin* "$dir"/basin; done`  
> `for dir in */; do mv "$dir"/*PourPoint* "$dir"/pour_point; done`  
> `for dir in */; do mv "$dir"/*Station* "$dir"/station; done`

Clean up the files: &gt;`rm *.qgz *.zip`

<!-- The file containing currently publicly available WSC station basin polygons can be retrieved by the command below.  The collection is less complete than the set available above, however the set above is not finalized/approved and is subject to revision.

>`wget -P source_data/WSC_data/  https://donnees.ec.gc.ca/data/water/products/national-hydrometric-network-basin-polygons/WSC_Basins.gdb.zip`  
>`unzip WSC_Basins.gdb.zip` -->

Create geojson objects as separate data structures of all basins:
&gt;`cd ~/hysets_validation/setup_scripts/`
&gt;`python process_wsc_basins.py`

Merge Stream Vectors for Burning
--------------------------------

**Optional**: if you want to re-derive basin groups, you can run the
script below, otherwise, the polygons describing the study region are
provided in the folder `processed_data/merged_basin_groups`.

Use the WSC SDA ids to group NHN features into the same polygons as used
in the regional groupings. In each NHN vector file
(i.e. `nhn_rhn_07aa000_shp_en.zip`, see National Hydrographic Network
section above) there is a layer containing the string `NLFLOW` which
represents the continuous stream network. Similar to how the
sub-subbasins were grouped above, use the same procedure to group the
stream vectors and save them as separate files. These will be used in
the FillBurn processing step for dem conditioning.

> `python group_stream_vectors.py` (**optional**)

**GLHYMPHS**

For porosity and permeability, HYSETS used the dataset from GLobal
HYdrogeology MaPS (Gleeson et al. 2014).

> `mkdir source_data/GLHYMPHS_data/`

You can’t use wget here unfortunately. Not straightforward to
auto-download because of license modal. Visit the link below, download,
and transfer to the data drive (~1.1 GB):

> `https://dataverse.scholarsportal.info/dataset.xhtml?persistentId=doi:10.5683/SP2/DLGXYO`

The file is called something like `GLHYMPS.zip` and you can transfer it
from your **local machine** to your cloud instance using `scp`:  
&gt;`scp /local/path/GLHYMPS.zip root@<cloud-ip-address>:/hysets_validation/source_data/`

Then unzip the file on the remote machine:  
&gt;`unzip GLHYMPS.zip`

**NALCMS**

Land use percentages (forest, grassland, crops, etc.) is derived from
the North American Land Change Monitoring System:

> `cd source_data/`  
> `mkdir NALCMS_data/`  
> `cd NALCMS_data/`  
> `wget http://www.cec.org/wp-content/uploads/wpallimport/files/Atlas/Files/2010nalcms30m/north_america_2010.zip > north_america_2010.zip`  
> `unzip north_america_2020.zip -d .`

Basin Delineation and Attribute Validation
------------------------------------------

This step represents the heavy lifting where large regions of DEM such
as the Liard, Peace, and and Fraser River basins are processed into flow
direction and flow accumulation at the highest available resolution.
Three tools were evaluated for the step of validating HYSETS basins:
[Whiteboxtools](python%20process_dem_by_basin.py),
[RichDEM](https://richdem.readthedocs.io/en/latest/), and
[Pysheds](https://mattbartos.com/pysheds/). Each have distinct feature
sets, but Pysheds was used here for the step of delineating a large set
of basins.

<!-- Note: the breach [depression function](https://jblindsay.github.io/ghrg/Whitebox/Help/BreachDepressions.html) run on the DEM is a bottleneck step.   -->

The final step is to validate the basin attributes derived in HYSETS (or
other dataset) using the set of stations whose catchment boundaries
intersect BC. The manual basin delineation step is the most
computationally intensive step of the validation process, and it’s
executed with the script `pysheds_derive_basin_polygons.py`

> `python pysheds_derive_basin_polygons.py`

GeoBC Digital Road Atlas
------------------------

Especially using higher resolution DEM, roads and bridges create issues
when delineating basins. Use the provincial road atlas in combination
with national hydrographic network streams to breach dems that don’t
delineate properly due to man-made obstructions.

Download the [Digital Road Atlas from the Gov’t of
BC](https://www2.gov.bc.ca/gov/content/data/geographic-data-services/topographic-data/roads),
or access the `.geojson` file using an ftp client (I used the
open-source software [Filezilla](https://filezilla-project.org/))
directly from the [ftp
link](ftp://ftp.geobc.gov.bc.ca/sections/outgoing/bmgs/DRA_Public/).

Additional Notes
----------------

### Trim Coastline DEM

![Example of trimming ocean out of coastline
DEM](../images/trimming_dem.png)

Note that the raw dem includes considerable areas of ocean that would
cause issues if included in any hydrologic processing. The
dem\_basin\_mapper.py script includes a shortcut where polygons were
pre-processed using a combination of coastline polygon sources to trim
ocean out of the DEM and create separate DEM files for each regional
group. In the image above, Haida Gwaii has been trimmed along the
coastline, while the coastline to the east shows the area that is
identified for trimming.

In the `setup_scripts` folder, there is a Jupyter Notebook file
`Trim_Coastal_Group_Polygons.ipynb` where the trimming process is shown.
The coastline geometries are derived from several sources, described
below.

Shape files describing the BC coastline, PNW (Washington state), and
southeast alaska basins are used to refine DEM masks from the watershed
region groups, and to include the area of southeast alaska not covered
in the NHN regional groupings.

**Southeast Alaska Basins**: get the geojson file and save it as
`USFS_Southeast_Alaska_Drainage_Basin__SEAKDB__Watersheds.geojson`:
&gt;https://hub.arcgis.com/datasets/seakgis::usfs-southeast-alaska-drainage-basin-seakdb-watersheds/about

**FWA Coastline Geometry**: get the geojson file for the BC coastline
and islands. Here you have to submit a request. The file is assembled
and distributed to your email automatically:

> https://catalogue.data.gov.bc.ca/dataset/freshwater-atlas-coastlines

**PNW Shoreline Shapefile** The southern extent of the coast is also
interrupted by the national border. Use the PNW coastline shape file to
crop the fraser basin ([*Shoreline for the Pacific Northwest
Region*](https://pubs.usgs.gov/ds/2006/236/download/shoreline_pacnw.zip)):

> https://pubs.usgs.gov/ds/2006/236/catalog.shtml\#GIS\_data\_table

### NHN Additional Notes

The NHN also contains many hydrologic features in detail, provided in
shape files by WLU, as described in the documentation linked above:

> *“It provides geospatial digital data compliant with the NHN Standard
> such as lakes, reservoirs, watercourses (rivers and streams), canals,
> islands, drainage linear network, toponyms or geographical names,
> constructions and obstacles related to surface waters, etc.”*

Data such as obstacles will be important for catchment delineation, as
obstacles such as bridges can interfere with the flow direction and
accumulation steps.

To access individual WLU attribute objects, use the following
convention:

> `https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_nhn_rhn/shp_en/XX/<filename>`

Where `XX` is the two-digit major drainage area (MDA) prefix (BC is
covered by codes 07, 08, 09, and 10). `<filename>` is for example
`nhn_rhn_07aa000_shp_en.zip`.

The feature files related to the WLU groups in the set covering BC were
downloaded and saved to the `processed_data/` folder when the
`process_hydrologic_regions.py` script was executed above.

An example of the features contained in these files is shown below
(shown imported in [QGIS](https://qgis.org/en/site/)):

![Example hydrologic features from NHN (from
QGIS)](img/nhn-features.png)

### Global River Classification (OPTIONAL)

https://ln.sync.com/dl/3d4952ac0/isuvquck-82fv4ca6-2vxh8nwm-fpeg9mra

Notes
-----

### Once processing is complete, you will need to transfer the results back to your local machine

Create folders and copy files using `scp` command:

> `scp setup_scripts/ account@1XX.XX.XXX.XX`. Here, `account@1XX.XX...`
> is the account name and IP address of the machine. You can specify a
> different path by adding a colon after the IP address (:),
> i.e. `account@XX.XX.XX:home/custom_path/data/`. The path must first be
> created on the destination machine.

To automate citation formatting for the README document.

> `pandoc -t markdown_strict -citeproc README-draft.md -o README.md --bibliography bib/bibliography.bib`

Gleeson, Tom, Nils Moosdorf, Jens Hartmann, and LPH Van Beek. 2014. “A
Glimpse Beneath Earth’s Surface: GLobal Hydrogeology Maps (Glhymps) of
Permeability and Porosity.” *Geophysical Research Letters* 41 (11):
3891–8.
